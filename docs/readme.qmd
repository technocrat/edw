---
title: "Code Walkthrough"
author:
  - name: Richard Careaga
    url: https://technocrat.rbind.io
date: last-modified
format:
  html:
    self-contained: true
    anchor-sections: true
    code-tools: true
    code-fold: true
    fig-width: 6
    fig-height: 4
    code-block-bg: "#f1f3f5"
    code-block-border-left: "#31BAE9"
    mainfont: Source Sans Pro
    theme: litera
    toc: true
    toc-depth: 3
    toc-location: left
    captions: true
    cap-location: margin
    table-captions: true
    tbl-cap-location: margin
    reference-location: margin

execute:
  warning: false
  message: false
  cache: false
---

<style type="text/css">
body{
  font-size: 16pt;
}
</style>

# Introduction

## Purpose

I describe a re-factoring of two scripts provided to read and process a large file of data collected for a normalized difference vegetation index. This was undertaken initially to address a problem with the existing code looping over a file in `bigmemory` with two scripts that were provided to me.

## Motivation for re-factoring

After extracting a portion of the data for testing, I found that the data could be extracted, converted to a `data.table` object and simple operations performed on it, such as summing each row. However, portions of the provided script crashed the `R` session. This led me to explore other out-of-RAM approaches, and I settled on batch processing the data from `csv` format.

## Difficulties with some desired variables

Only some of the desired variables could be calculated.

```{r}
#| echo: true
receiver <- data.table::data.table(
# pix is the record id added corresponding to original row order
  pix = as.integer(),
  # variables derived from the smoothed splines
  lq = as.double(),
  uq = as.double(),
  mean.evi = as.double(),
  sd.evi = as.double(),
  sum.evi = as.double(),
  amplitude = as.double(),
  devi.ss = as.double(),
  devi.sss = as.double(),
  # a constant from cons.R
  L.JDAY = as.integer(),
  # variables derived from from scaled data
  NA.length = as.integer(),
  ICE.length = as.integer()
)
```

The remaining variables were undefined or incompletely defined in the scripts provided. 

## Analytic approach

> $f(x) = y$

*x* is an object to hand, *y* is the object desired and *f* is the object to transform one to the other. Any of the objects may be composite.

## x

*x* is a `bigmemory` object in the `data` directory, comprised of a principal data object `ndvi.bin` and an associated metadata object `ndvi.desc`. *x* is a matrix of approximately 2 million integer observations at 780 dates. The present problem arose as a result of difficulty in reading in *x*, processing it in parallel and resolving various error messages.

`JDAY.x` is an auxiliary object in the `obj` directory that was also provided to me.

## y

*y* is a data frame which is to contain summary statistics of the data in *x*.

## f

*f* will be a composed function object to make the transformation.

# Project organization

The project is organized as an RStudio `.Rproj` folder with a `git` [repository](https://github.com/technocrat/edw.git). The repo excludes the `data` and `obj` folders due to size limitation. To reproduce the repository, clone in the terminal and then add the omitted directories.

```
git clone https://github.com/technocrat/edw.git
cd edw
mkdir data obj
```

Place the `ndvi` files in the `data` directory and the `JDAY.x.rds` in the `obj` directory.

# External dependency

One operation, splitting the imported `ndvi` object into a directory of `csv` files is better done outside `R`, using a `shell` program, which requires the installation of the two Linux libraries from the command line

```
sudo apt-get install moreutils
sudo apt-get install parallel
```

Followed by creating a directory under folder `obj`

```
mkdir chunks
```

If there is trouble with the installation, please let me know and I will find an alternative in `R`.

# R resources

## Base

R.4.1.2 is recommended but is only necessary if any of the packages relied on require it.

## Libraries required

```
library(bigmemory)    # conversion of existing ndvi files
library(data.table)   # faster data frame implementation
library(here)         # refer to files relative to the edw.Rproj directory
library(lubridate)    # date handling
library(tictoc)       # used for testing run time (optional)
```

# Preprocessing

## Conversion to a single, large `csv` file

The `bigmemory` data is first read in from `data` folder and then and written back in `csv` format to the `obj` directory using the `prep.R` script, which adds an identifier column based on the sequential row number. This will serve to keep the data in its original order.

## Splitting of the single, large `csv` file to multiple `csv` files

From the terminal in the data directory

```
cd chunks
cat ../d.csv | parallel --header : --pipe -N2000 'cat > chunk_{#}.csv'
```

The files, except for the last, are of `dim` 500 781 in 4,186 files.

# Pattern for looping over a single `csv` file of 500 records. `s` is scaled data.


```{r}
source(here::here("R/prepare.R"))
s <- fread(here("scaled.csv"))
make_sss <- function(x){
  # adjust col width to reflect removal of first and last values
  #sss <- matrix(0,chunk_size,col_width - 2) 
  for(i in 1:chunk_size)  {
    # TODO fix hardwired 781
    sss_both = smooth.spline(JDAY.x[-c(2,col_width - 1)],
                         x[i,-c(1:2,781)],
                         spar = 0.3)
    sss.x = sss_both$x
    sss.y = sss_both$y
    d1 = predict(sss_both, sss.x, deriv = 1)$y
    d1 = (d1 - mean(d1)) / sd(d1)
    d2 = predict(sss_both, sss.x, deriv = 2)$y
    d2 = (d2 - mean(d2)) / sd(d2)
    # deviation between smoother spline and data
    devi.sss = mean(unlist(abs(x[i,-c(1:2,781)] - sss.y)))  
    return(list(d1,d2,devi.sss))
    }
}
make_sss(s)
```

## Pattern for looping over a folder of multiple `csv` files of 500 records each. `s` is scaled data.

```
chunks <- dir(here("obj/chunks"), pattern = "\\.csv$", full.names = TRUE)

for(i in seq_along(files)){
  the_pix[i] = fread(files[i][1])
}
```

## Pattern to append inner and outer loops to a data.table

**TODO**

# Workflow summary

1. Convert `ndvi.desc` to `csv` using the `prep.R` script
2. Split the resulting file using the rust `xvs` script in the `code` directory
3. Scale the raw `csv` data using the `stl.R` script
4. Extract ice/snow points using the `find_frozen` function using the scaled `csv` data from step 3
5. Extract number of NA values using the scaled `csv` data from step 4
6. Interpolate missing values in the scaled data using the `interpolate` function 
7. Smooth the interpolated data with a filtered running mean using the `make_splines` function
8. Smooth splines on smoothed data using the `make_smooth` function
9. Take derivatives of smooth splines using the `make_sss` function
